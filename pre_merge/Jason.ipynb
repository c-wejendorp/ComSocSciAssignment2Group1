{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ass2 / Part 3 / Exercise 4\n",
    "I need to complete have access to the following\n",
    "- The assignment of each author to their network community, and the degree of each author (Week 6, Exercise 4). This can be stored in a dataframe or in two dictionaries, as you prefer.\n",
    "- The tokenized abstract dataframe (Week 7, Exercise 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6, Exercise 4\n",
    "The assignment of each author to their network community, and the degree of each author\n",
    "\n",
    "I need access to the network created in week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/graph.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnetworkx\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnx\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mdata/graph.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m      6\u001b[0m G \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mnode_link_graph(data) \n",
      "File \u001b[1;32mc:\\Users\\Jason\\miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/graph.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "f = open('data/graph.json')\n",
    "data = json.load(f)\n",
    "# G = nx.node_link_graph(data) \n",
    "# print(f\"The number of nodes before the GCC has been found: {len(list(G.nodes))}\")\n",
    "# largest_cc = max(nx.connected_components(G), key=len)\n",
    "# # update graph to only include the larget connected component. \n",
    "# G = G.subgraph(largest_cc)\n",
    "# print(f\"The number of nodes after the GCC has been found: {len(list(G.nodes))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7, Exercise 1-2\n",
    "Seems the entire week is neccesary to complete this exercise\n",
    "\n",
    "\n",
    "## Exercise 1: Tokenization and Zipf's law\n",
    "Consider the list of abstracts of Computational Social Science papers. You shall take the paperIds of Computational Social Science papers (from Week 4), then go back to your abstract dataframe (Week 2) to get the abstracts only for those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1125521, 7)\n",
      "(1125521, 2)\n"
     ]
    }
   ],
   "source": [
    "# Import authorDataSet.pkl from dowlnoad folder\n",
    "import pickle \n",
    "with open('C:/Users/Jason/Downloads/fixedPaperDataSet.pkl', 'rb') as f:\n",
    "    papersDataSet = pickle.load(f)\n",
    "with open('C:/Users/Jason/Downloads/paperAbstractDataSet.pkl', 'rb') as f:\n",
    "    abstractDataSet = pickle.load(f)\n",
    "print(papersDataSet.shape)\n",
    "print(abstractDataSet.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Tokenization\n",
    "NB: The abstractDataSet only contains abstracts for the papers in the paperDataSet. Also, some are None, so are filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenize abstracts by following section 3.7 https://www.nltk.org/book/ch03.html\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "urls = '\\S+www\\S+\\w'    # remove urls by searching for www\n",
    "symbols = '[^\\w\\s]'  # remove punctuation\n",
    "numbers = '\\d+' # remove numbers\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def tokenize(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    text = text.lower()\n",
    "    text = re.sub(fr'{symbols}|{urls}|{numbers}','',text)\n",
    "    text = [word for word in text.split() if word not in stop_words]\n",
    "    return text\n",
    "\n",
    "# text = abstractDataSet['papersAbstract'][123]\n",
    "# text = \"Hello there mister i, how are you doing today? www.pornhub.com. 24. I'm doing great.\".lower()\n",
    "# tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column in abstractDataSet for tokenized abstracts and skip rows with no abstracts\n",
    "_abstractDataSet = abstractDataSet.copy()[0:100]\n",
    "_abstractDataSet['tokenizedAbstract'] = _abstractDataSet['papersAbstract'].apply(tokenize)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3) Single list and top 50 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 47), ('systems', 32), ('education', 32), ('learning', 31), ('system', 30), ('students', 29), ('information', 27), ('die', 25), ('der', 20), ('und', 20), ('work', 19), ('design', 19), ('paper', 19), ('new', 19), ('educational', 19), ('research', 18), ('process', 18), ('engineering', 18), ('school', 17), ('different', 15), ('study', 15), ('processes', 15), ('schools', 15), ('machine', 14), ('analysis', 14), ('digital', 14), ('management', 14), ('development', 14), ('higher', 14), ('project', 14), ('used', 13), ('social', 13), ('important', 13), ('users', 12), ('three', 12), ('factors', 12), ('context', 12), ('based', 11), ('also', 11), ('user', 11), ('support', 11), ('traces', 11), ('technology', 11), ('working', 10), ('results', 10), ('show', 10), ('use', 10), ('order', 10), ('trust', 10), ('decisionmaking', 10)]\n"
     ]
    }
   ],
   "source": [
    "# Gather all tokenized abstracts into one listÂ¨\n",
    "tokenizedAbstracts = [] \n",
    "for index, row in _abstractDataSet.iterrows():\n",
    "    if row['tokenizedAbstract'] is not None:\n",
    "        tokenizedAbstracts.append(row['tokenizedAbstract'])\n",
    "\n",
    "# Create a dictionary of all words and their frequency\n",
    "from collections import Counter\n",
    "wordCount = Counter()\n",
    "for abstract in tokenizedAbstracts:\n",
    "    wordCount.update(abstract)\n",
    "\n",
    "# Create a list of the 50 most common words\n",
    "mostCommonWords = wordCount.most_common(50)\n",
    "print(mostCommonWords)\n",
    "\n",
    "# Print the 50 most common words # HACK Paste this list into markdown\n",
    "# print(\"50 most common words:\")\n",
    "# for word in mostCommonWords:\n",
    "#     print(word[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5) Zipf's law and random distribution (Skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_token_frequency(tokens):\n",
    "    wordCount = Counter()\n",
    "    for abstract in tokens:\n",
    "        wordCount.update(abstract)\n",
    "    \n",
    "    # Plot Zipf's law\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import seaborn as sns\n",
    "    sns.set()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(\"Zipf's law\")\n",
    "    plt.xlabel(\"Rank\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    # Logarithmic scale\n",
    "    plt.yscale('log')\n",
    "    plt.plot(np.arange(1, len(wordCount)+1), sorted(wordCount.values(), reverse=True))\n",
    "    plt.show()\n",
    "\n",
    "plot_token_frequency(tokenizedAbstracts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Bigrams and contingency tables.\n",
    "### 1) Bigrams\n",
    "Find the list of bigrams in each of the abstracts. Store all the bigrams in a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bigrams from tokenized abstracts\n",
    "from nltk import bigrams\n",
    "bigram_tokens = [bigram for abstract in tokenizedAbstracts for bigram in bigrams(abstract)]\n",
    "bigram_tokens[:3]\n",
    "\n",
    "# Get the number of times the bigram occurs in the corpus (relevant for later)\n",
    "bigramCount = Counter()\n",
    "for bigram in bigram_tokens:\n",
    "    bigramCount.update([bigram])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) contingency table and p-value\n",
    "For each unique bigram in your list:\n",
    "- compute the corresponding contingency table (see the theory just above)\n",
    "- compute the p-value associated to the Chi-squared test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[(('machine', 'learning'), 12), (('digital', 'traces'), 11), (('information', 'systems'), 10), (('higher', 'education'), 10), (('recommendation', 'system'), 6), (('teaching', 'learning'), 6), (('feder', 'funds'), 6), (('service', 'level'), 5), (('social', 'scientists'), 4), (('choropleth', 'maps'), 4)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute contingency table of bigrams\n",
    "\n",
    "# For now look at the first bigram\n",
    "bigram_sample = bigram_tokens[0]\n",
    "\n",
    "nii = bigramCount[bigram_sample]\n",
    "# Counter number of occurrences of the first word in the bigram\n",
    "ni = wordCount[bigram_sample[0]] \n",
    "# Get most common bigrams\n",
    "print(bigramCount.most_common(10))\n",
    "\n",
    "\n",
    "bigram_count = wordCount[bigram_sample[0]]\n",
    "bigram_count\n",
    "\n",
    "# Get unique bigrams\n",
    "# unique_bigrams = set(bigram_tokenz)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('gain', 'objective')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(bigram_tokenz))\n",
    "len(unique_bigrams)\n",
    "# get some element in set\n",
    "list(unique_bigrams)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
