{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Community detection on the network of Computational Social Scientists.\n",
    "## Week 6, Exercise 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider the network you built in Week 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import netwulf as nw\n",
    "from netwulf import visualize\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "f = open('data/graph.json')\n",
    "data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes before the GCC has been found: 2162\n",
      "The number of nodes after the GCC has been found: 1271\n"
     ]
    }
   ],
   "source": [
    "G = nx.node_link_graph(data) \n",
    "print(f\"The number of nodes before the GCC has been found: {len(list(G.nodes))}\")\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "# update graph to only include the larget connected component. \n",
    "G = G.subgraph(largest_cc)\n",
    "print(f\"The number of nodes after the GCC has been found: {len(list(G.nodes))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the Python Louvain-algorithm implementation to find communities. How many communities do you find? What are their sizes? Report the value of modularity found by the algorithm. Is the modularity significantly different than 0?\n",
    "> The modularity is 0.899, which is significantly different than 0. Modularity measures <> and is found within the range [-1/2, 1]. Thus, this partition is well done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of communities found: 33\n",
      "\n",
      "Community : Count\n",
      "0 : 27 | 1 : 29 | 2 : 77 | 3 : 30 | \n",
      "4 : 29 | 5 : 52 | 6 : 46 | 7 : 65 | \n",
      "8 : 80 | 9 : 22 | 10 : 29 | 11 : 86 | \n",
      "12 : 38 | 13 : 120 | 14 : 2 | 15 : 53 | \n",
      "16 : 43 | 17 : 30 | 18 : 22 | 19 : 29 | \n",
      "20 : 76 | 21 : 18 | 22 : 17 | 23 : 5 | \n",
      "24 : 19 | 25 : 31 | 26 : 41 | 27 : 37 | \n",
      "28 : 34 | 29 : 18 | 30 : 20 | 31 : 29 | \n",
      "32 : 17 | \n",
      "\n",
      "The modularity of the graph is: 0.8985766049389848\n"
     ]
    }
   ],
   "source": [
    "import community\n",
    "\n",
    "\n",
    "# Find all communities in the graph\n",
    "partition = community.best_partition(G)  # This returns inconsistent results\n",
    "num_communities = len(set(partition.values()))\n",
    "\n",
    "# Number of communities\n",
    "print(f\"Number of communities found: {num_communities}\")\n",
    "_, counts = np.unique(list((partition.values())), return_counts=True)\n",
    "\n",
    "# Community sizes\n",
    "print(\"\\nCommunity : Count\")\n",
    "kek = 4\n",
    "for i in range(0, len(counts), kek):\n",
    "    for com, c in zip(list(range(i, i+kek)), counts[i:i+kek]):\n",
    "        print(f\"{com} : {c} \", end=\"| \")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Modularity\n",
    "modularity = community.modularity(partition, G)\n",
    "print(f\"\\nThe modularity of the graph is: {modularity}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you are curious, you can also try the Infomap algorithm. Go to [this page]. (https://mapequation.github.io/infomap/python/). It's harder to install, but a better community detection algorithm. You can read about it in advanced topics 9B."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize the network, using netwulf (see Week 5). This time assign each node a different color based on their community. Describe the structure you observe.\n",
    "> The structure looks fine as hell. XYZ PROPER TERMS HERE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INCLUDE SEXY SCREEN SHOT HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make sure you save the assignment of authors to communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, partition, name=\"group\")  # group controls color"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: TF-IDF and the Computational Social Science communities.\n",
    "The goal for this exercise is to find the words charachterizing each of the communities of Computational Social Scientists.\n",
    "> Student critic: Calculate TF-IDF for each word in each community. THEN, find top 10 lists across community. As you'll see, there will be some redundancy, because we strictly follow your instructions. SUGGESTION: For better coding practice, have us write specific functions that we can call in the main code to answer your questions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Check wikipedia for TF-IDF.\n",
    "Explain in your own words the point of TF-IDF. What does TF and IDF stand for?\n",
    "> Short for `term frequencyâ€“inverse document frequency`, it is a method applied in information retrieval (IR) that down weighs frequent terms. This is important, because word frequencies are relatively logaritmic cf. Zipf's law, and we want to avoid that the most frequent words dominate the analysis. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Community abstracts\n",
    "Now, we want to find out which words are important for each community, so we're going to create several **large documents, one for each community**. Each document includes all the tokens of abstracts written by members of a given community.\n",
    "\n",
    "- Consider a community c\n",
    "- Find all the abstracts of papers written by a (ALL) member(S) of community c.\n",
    "- Create a long array that stores all the abstract tokens\n",
    "- Repeat for all the communities.\n",
    "\n",
    "> This is quite the task. For completeness the tokenized abstracts are genereated here with code from week 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer code written in week7\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "urls = '\\S+www\\S+\\w'    # remove urls by searching for www\n",
    "symbols = '[^\\w\\s]'     # remove punctuation\n",
    "numbers = '\\d+'         # remove numbers\n",
    "stop_words = stopwords.words('english')\n",
    "ps = PorterStemmer()    # Stemming\n",
    "\n",
    "def tokenize(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    text = text.lower()\n",
    "    text = re.sub(fr'{symbols}|{urls}|{numbers}','',text)\n",
    "    text = [ps.stem(word) for word in text.split() if word not in stop_words] \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(969493, 8)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle  # HACK DELETE LATER\n",
    "\n",
    "# abstracDataSet contains paperId and their abstracts\n",
    "# paperDataSet contains paperId\ttitle\tyear\texternalId.DOI\tcitationCount\tfields\tauthorIds\tauthor_field\n",
    "\n",
    "with open('data/paperAbstractDataSet.pkl', 'rb') as f:\n",
    "    abstractDataSet = pickle.load(f)\n",
    "abstractDataSet=abstractDataSet.drop_duplicates(subset=['papersId'])\n",
    "\n",
    "with open('data/ccs_papers.pkl', 'rb') as f:\n",
    "    paperDataSet = pickle.load(f)\n",
    "\n",
    "paperDataSet.shape\n",
    "# 0.5 min"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We need access to the papers written by the authors in the graph. We have a paperDataSet that has up to a million entrees. This is cut down by filtering out papers where none of the contributors exists in the graph. The `explode` command is used, because the `authors` column is a list of authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dict(G.nodes)\n",
    "valid = paperDataSet['authorIds'].apply(lambda x: any(elem in temp for elem in x))\n",
    "papers = paperDataSet[valid]         # Filter out papers with authors not in graph\n",
    "papers = papers.explode('authorIds') # Explode the authorIds column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now collect all unique paperIDs for each community using sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1271/1271 [00:16<00:00, 77.73it/s]\n"
     ]
    }
   ],
   "source": [
    "communityPaperIDs = [set() for _ in range(num_communities)]\n",
    "for node in tqdm(G.nodes(data=True)):\n",
    "    author = node[0]\n",
    "    writtenPapers=papers[papers[\"authorIds\"].isin([author])][\"paperId\"]\n",
    "    community = partition[author]\n",
    "    communityPaperIDs[community].update(writtenPapers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the paperIDS, the corresponding abstracts are found in the abstracts dataset. \n",
    "The abstracts are then tokenized and stored in a list for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [06:54, 12.57s/it]\n"
     ]
    }
   ],
   "source": [
    "corpus = [[] for _ in range(len(communityPaperIDs))]\n",
    "\n",
    "\n",
    "for i, paperIDs in tqdm(enumerate(communityPaperIDs)):\n",
    "    abstracts=abstractDataSet[abstractDataSet[\"papersId\"].isin(paperIDs)][\"papersAbstract\"]\n",
    "\n",
    "    abstracts = abstracts.dropna()    # Drop all rows with None values\n",
    "    abstracts.apply(lambda x: corpus[i].extend(tokenize(x)))\n",
    "\n",
    "# 7 minutes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) Calculate TF\n",
    "Now, we're ready to calculate the TF for each word. Use the method of your choice to find the top 5 terms within the top 5 communities (by number of authors).\n",
    "\n",
    "> First isolate the tokens of the top 5 communities. Then Cacluate TF for each token in each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 communities: [13 11  8  2 20]\n"
     ]
    }
   ],
   "source": [
    "# First find top 5 comminutes by size\n",
    "top5communities = np.argsort(counts)[-5:][::-1]\n",
    "# top5tokens = [communityTokens[x] for x in top5communities]\n",
    "print(\"Top 5 communities:\", top5communities)\n",
    "corpus5 = [corpus[x] for x in top5communities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tf for each community\n",
    "from collections import Counter\n",
    "TF5 = [Counter(tokens) for tokens in corpus5]  # non-normalized term frequency\n",
    "\n",
    "# Normliaze the tf\n",
    "for i, tf in enumerate(TF5):\n",
    "    for key in tf:\n",
    "        tf[key] /= len(corpus5[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 terms for each community:\n",
      "13) : ('use', 'user', 'social', 'studi', 'data')\n",
      "11) : ('use', 'user', 'social', 'data', 'design')\n",
      "8) : ('use', 'social', 'data', 'inform', 'user')\n",
      "2) : ('use', 'network', 'data', 'social', 'model')\n",
      "20) : ('model', 'use', 'algorithm', 'data', 'inform')\n"
     ]
    }
   ],
   "source": [
    "# Find the top 5 terms for each community\n",
    "top5terms = [tf.most_common(5) for tf in TF5]\n",
    "top5terms = [list(zip(*terms))[0] for terms in top5terms] # Extract the terms\n",
    "\n",
    "print(\"Top 5 terms for each community:\")\n",
    "for i, terms in zip(top5communities, top5terms):\n",
    "    print(f\"{i}) : {terms}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe similarities and differences between the communities.\n",
    "> Similarities include **social, data, and information**. \n",
    "\n",
    "> Differences include **network, time, show, and results**.\n",
    "\n",
    "- Why aren't the TFs not necessarily a good description of the communities?\n",
    "> TF alone does not consider the significance of a word, i.e. words that in general appear often will unfailry score high. \n",
    "\n",
    "- Next, we calculate IDF for every word.\n",
    "> The IDF is calculated where it is assumed that each community has one document, such that N = number of communities. First determine the number of communities that contain a given word. Then calculate the IDF for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IDF for every term\n",
    "# IDF = log(N / n), \n",
    "# where N is the number of communities and n is the number of communities that contain the term\n",
    "\n",
    "IDF5 = Counter()\n",
    "N = len(corpus5)\n",
    "\n",
    "# For each term, count the number of communities that contain it\n",
    "for c in corpus5:\n",
    "    for term in set(c):\n",
    "        IDF5[term] += 1\n",
    "\n",
    "# Compute the IDF\n",
    "IDF5 = {key: np.log(N / value) for key, value in IDF5.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What base logarithm did you use? Is that important?\n",
    "> Natural logirithm. Not important, we are just interested in projecting onto the log scale.\n",
    "\n",
    "## 4) TF-IDF\n",
    "We're ready to calculate TF-IDF. Do that for the top 9 communities (by number of authors). Then for each community:\n",
    "\n",
    "- List the 10 top TF words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 9 communities: [13 11  8  2 20  7 15  5  6]\n",
      "Top 10 terms for each community:\n",
      "13) : ('use', 'user', 'social', 'studi', 'data', 'research', 'work', 'system', 'inform', 'model')\n",
      "11) : ('use', 'user', 'social', 'data', 'design', 'system', 'research', 'inform', 'studi', 'commun')\n",
      "8) : ('use', 'social', 'data', 'inform', 'user', 'algorithm', 'studi', 'time', 'model', 'result')\n",
      "2) : ('use', 'network', 'data', 'social', 'model', 'system', 'mobil', 'inform', 'studi', 'result')\n",
      "20) : ('model', 'use', 'algorithm', 'data', 'inform', 'result', 'problem', 'show', 'effect', 'learn')\n",
      "7) : ('use', 'user', 'network', 'data', 'system', 'social', 'model', 'inform', 'result', 'show')\n",
      "15) : ('use', 'network', 'social', 'data', 'model', 'studi', 'inform', 'research', 'result', 'polit')\n",
      "5) : ('network', 'use', 'data', 'model', 'social', 'user', 'inform', 'studi', 'result', 'commun')\n",
      "6) : ('use', 'data', 'model', 'propos', 'social', 'method', 'user', 'network', 'result', 'studi')\n"
     ]
    }
   ],
   "source": [
    "# First find top 9 communities by size\n",
    "N = 9\n",
    "top9communities = np.argsort(counts)[-N:][::-1]\n",
    "print(f\"Top {N} communities:\", top9communities)\n",
    "corpus9 = [corpus[x] for x in top9communities]\n",
    "\n",
    "# Calculate the tf for each community\n",
    "from collections import Counter\n",
    "TF9 = [Counter(tokens) for tokens in corpus9]  # non-normalized term frequency\n",
    "\n",
    "# Normliaze the tf\n",
    "for i, tf in enumerate(TF9):\n",
    "    for key in tf:\n",
    "        tf[key] /= len(corpus9[i])\n",
    "\n",
    "top10terms9 = [tf.most_common(10) for tf in TF9]\n",
    "top10terms9 = [list(zip(*terms))[0] for terms in top10terms9] # Extract the terms\n",
    "\n",
    "print(\"Top 10 terms for each community:\")\n",
    "for i, terms in zip(top9communities, top10terms9):\n",
    "    print(f\"{i}) : {terms}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List the 10 top TF-IDF words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 terms for each community:\n",
      "13) : ('dram', 'codemix', 'ictd', 'sci', 'phish', 'streamit', 'transliter', 'mooc', 'hindi', 'bangalor')\n",
      "11) : ('earthworm', 'microfilm', 'hci', 'psl', 'searcher', 'ubuntu', 'vape', 'odk', 'hcai', 'spreadsheet')\n",
      "8) : ('corros', 'maritim', 'anod', 'gull', 'calv', 'childless', 'zika', 'qatar', 'childbear', 'eubalaena')\n",
      "2) : ('mbb', 'aria', 'latrin', 'tota', 'deli', 'saper', 'pfpr', 'roam', 'deaggreg', 'inod')\n",
      "20) : ('dpp', 'bidder', 'Ã§', 'estimand', 'multirobot', 'wager', 'ewa', 'pprl', 'actr', 'timber')\n",
      "7) : ('bitext', 'superp', 'diacrit', 'rumour', 'multicast', 'uma', 'tl', 'anycast', 'crosslanguag', 'gaminganywher')\n",
      "15) : ('mdd', 'ora', 'lithium', 'gasolin', 'antidepress', 'bipolar', 'bd', 'roosevelt', 'olanzapin', 'carley')\n",
      "5) : ('smallsid', 'eip', 'dasymetr', 'knot', 'ghsl', 'nonconserv', 'hashcod', 'tvg', 'ssg', 'antisci')\n",
      "6) : ('reid', 'spancor', 'qatar', 'influenzanet', 'giorno', 'dtd', 'adblock', 'antiadblock', 'samoa', 'queryflow')\n"
     ]
    }
   ],
   "source": [
    "IDF9 = Counter()\n",
    "N = len(corpus9)\n",
    "\n",
    "# For each term, count the number of communities that contain it\n",
    "for c in corpus9:\n",
    "    for term in set(c):\n",
    "        IDF9[term] += 1\n",
    "\n",
    "# Compute the IDF\n",
    "IDF9 = {key: np.log(N / value) for key, value in IDF9.items()}\n",
    "\n",
    "# Compute TF-IDF for each community\n",
    "TFIDF9 = [Counter() for _ in range(N)]\n",
    "for i, tf in enumerate(TF9):\n",
    "    for term in tf:\n",
    "        TFIDF9[i][term] = tf[term] * IDF9[term]\n",
    "\n",
    "# Extract the top 10 terms for each community\n",
    "top10terms9 = [tf.most_common(10) for tf in TFIDF9]\n",
    "top10terms9 = [list(zip(*terms))[0] for terms in top10terms9] # Extract the terms\n",
    "\n",
    "print(\"Top 10 terms for each community:\")\n",
    "for i, terms in zip(top9communities, top10terms9):\n",
    "    print(f\"{i}) : {terms}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- List the top 3 authors (by degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 authors for each community:\n",
      "13) : ['Joyojeet Pal', 'Priyanka Chandra', 'Vaishnav Kameswaran']\n",
      "11) : ['Munmun De Choudhury', 'Sarita Yardi Schoenebeck', 'Neha Kumari Pawan Kumar']\n",
      "8) : ['Ingmar G. Weber', 'Masoomali Fatehkia', 'Ridhi Kashyap']\n",
      "2) : ['Alexander Sandy Pentland', 'Iyad Rahwan', 'Johannes Bjelland']\n",
      "20) : ['Duncan J. Watts', 'Markus M. Mobius', 'Sharad Chandra Goel']\n",
      "7) : ['Haewoon Kwak', 'Daniele Quercia', 'Krishna P. Gummadi']\n",
      "15) : ['David M. J. Lazer', 'Jon Green', 'Katherine Ognyanova']\n",
      "5) : ['Michael D. Conover', \"M'arton Karsai\", 'Filippo Menczer']\n",
      "6) : ['Yelena A Mejova', 'Kyriaki Kalimeri', 'Daniela Paolotti']\n"
     ]
    }
   ],
   "source": [
    "# Determine degree of all nodes\n",
    "import pandas as pd\n",
    "degree = dict(G.degree())\n",
    "nx.set_node_attributes(G, degree, name=\"degree\") \n",
    "df=pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')\n",
    "\n",
    "# Extract top 3 authors for each community by degree\n",
    "top3authors = []\n",
    "for i in range(num_communities):\n",
    "    authors = [node for node in G.nodes(data=True) if node[1]['group'] == i]\n",
    "    top3authors.append(sorted(authors, key=lambda x: x[1]['degree'], reverse=True)[:3])\n",
    "    # Extract their names\n",
    "    top3authors[i] = [x[1]['name'] for x in top3authors[i]]\n",
    "\n",
    "print(\"Top 3 authors for each community:\")\n",
    "for i in top9communities:\n",
    "    authors = top3authors[i]\n",
    "    print(f\"{i}) : {authors}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are these 10 words more descriptive of the community? If yes, what is it about IDF that makes the words more informative?\n",
    "> Looking at the output from the code just before the one above... Yes! As opposed to before where there was large overlap, now they seem very exclusive to each community. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
