{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Community detection on the network of Computational Social Scientists.\n",
    "## Week 6, Exercise 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider the network you built in Week 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import netwulf as nw\n",
    "from netwulf import visualize\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "f = open('ndata/graph.json')\n",
    "data = jso.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes before the GCC has been found: 2162\n",
      "The number of nodes after the GCC has been found: 1271\n"
     ]
    }
   ],
   "source": [
    "G = nx.node_link_graph(data) \n",
    "print(f\"The number of nodes before the GCC has been found: {len(list(G.nodes))}\")\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "# update graph to only include the larget connected component. \n",
    "G = G.subgraph(largest_cc)\n",
    "print(f\"The number of nodes after the GCC has been found: {len(list(G.nodes))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the Python Louvain-algorithm implementation to find communities. How many communities do you find? What are their sizes? Report the value of modularity found by the algorithm. Is the modularity significantly different than 0?\n",
    "> The modularity is 0.899, which is significantly different than 0. Modularity measures <> and is found within the range [-1/2, 1]. Thus, this partition is well done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of communities found: 31\n"
     ]
    }
   ],
   "source": [
    "import community\n",
    "\n",
    "\n",
    "# Find all communities in the graph\n",
    "partition = community.best_partition(G)  # This returns inconsistent results\n",
    "num_communities = len(set(partition.values()))\n",
    "\n",
    "# Number of communities\n",
    "print(f\"Number of communities found: {num_communities}\")\n",
    "_, counts = np.unique(list((partition.values())), return_counts=True)\n",
    "\n",
    "# Community sizes\n",
    "print(\"\\nCommunity : Count\")\n",
    "kek = 4\n",
    "for i in range(0, len(counts), kek):\n",
    "    for com, c in zip(list(range(i, i+kek)), counts[i:i+kek]):\n",
    "        print(f\"{com} : {c} \", end=\"| \")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Modularity\n",
    "modularity = community.modularity(partition, G)\n",
    "print(f\"\\nThe modularity of the graph is: {modularity}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you are curious, you can also try the Infomap algorithm. Go to [this page]. (https://mapequation.github.io/infomap/python/). It's harder to install, but a better community detection algorithm. You can read about it in advanced topics 9B."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize the network, using netwulf (see Week 5). This time assign each node a different color based on their community. Describe the structure you observe.\n",
    "> The structure looks fine as hell. XYZ PROPER TERMS HERE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INCLUDE SEXY SCREEN SHOT HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make sure you save the assignment of authors to communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, partition, name=\"group\")  # group controls color"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: TF-IDF and the Computational Social Science communities.\n",
    "The goal for this exercise is to find the words charachterizing each of the communities of Computational Social Scientists."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Check wikipedia for TF-IDF.\n",
    "Explain in your own words the point of TF-IDF. What does TF and IDF stand for?\n",
    "> Short for `term frequencyâ€“inverse document frequency`, it is a method applied in information retrieval (IR) that down weighs frequent terms. This is important, because word frequencies are relatively logaritmic cf. Zipf's law, and we want to avoid that the most frequent words dominate the analysis. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Community abstracts\n",
    "Now, we want to find out which words are important for each community, so we're going to create several **large documents, one for each community**. Each document includes all the tokens of abstracts written by members of a given community.\n",
    "\n",
    "- Consider a community c\n",
    "- Find all the abstracts of papers written by a (ALL) member(S) of community c.\n",
    "- Create a long array that stores all the abstract tokens\n",
    "- Repeat for all the communities.\n",
    "\n",
    "> This is quite the task. For completeness the tokenized abstracts are genereated here with code from week 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jason\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer code written in week7\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "urls = '\\S+www\\S+\\w'    # remove urls by searching for www\n",
    "symbols = '[^\\w\\s]'     # remove punctuation\n",
    "numbers = '\\d+'         # remove numbers\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def tokenize(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    text = text.lower()\n",
    "    text = re.sub(fr'{symbols}|{urls}|{numbers}','',text)\n",
    "    text = [word for word in text.split() if word not in stop_words]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(969493, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle  # HACK DELETE LATER\n",
    "\n",
    "# abstracDataSet contains paperId and their abstracts\n",
    "# paperDataSet contains paperId\ttitle\tyear\texternalId.DOI\tcitationCount\tfields\tauthorIds\tauthor_field\n",
    "\n",
    "with open('data/paperAbstractDataSet.pkl', 'rb') as f:\n",
    "    abstractDataSet = pickle.load(f)\n",
    "abstractDataSet=abstractDataSet.drop_duplicates(subset=['papersId'])\n",
    "\n",
    "with open('data/ccs_papers.pkl', 'rb') as f:\n",
    "    paperDataSet = pickle.load(f)\n",
    "\n",
    "paperDataSet.shape\n",
    "# 0.5 min"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We need access to the papers written by the authors in the graph. We have a paperDataSet that has up to a million entrees. This is cut down by filtering out papers where none of the contributors exists in the graph. The `explode` command is used, because the `authors` column is a list of authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dict(G.nodes)\n",
    "valid = paperDataSet['authorIds'].apply(lambda x: any(elem in temp for elem in x))\n",
    "papers = paperDataSet[valid]         # Filter out papers with authors not in graph\n",
    "papers = papers.explode('authorIds') # Explode the authorIds column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now collect all unique paperIDs for each community using sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "communityPaperIDs = [set() for _ in range(num_communities)]\n",
    "for node in tqdm(G.nodes(data=True)):\n",
    "    author = node[0]\n",
    "    writtenPapers=papers[papers[\"authorIds\"].isin([author])][\"paperId\"]\n",
    "    community = partition[author]\n",
    "    communityPaperIDs[community].update(writtenPapers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With the paperIDS, the corresponding abstracts are found in the abstracts dataset. \n",
    "The abstracts are then tokenized and stored in a list for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [01:09,  2.18s/it]\n"
     ]
    }
   ],
   "source": [
    "communityTokens = [[] for _ in range(len(communityPaperIDs))]\n",
    "\n",
    "\n",
    "for i, paperIDs in tqdm(enumerate(communityPaperIDs)):\n",
    "    abstracts=abstractDataSet[abstractDataSet[\"papersId\"].isin(paperIDs)][\"papersAbstract\"]\n",
    "\n",
    "    abstracts = abstracts.dropna()    # Drop all rows with None values\n",
    "    abstracts.apply(lambda x: communityTokens[i].extend(tokenize(x)))\n",
    "\n",
    "# 1 minute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) Calculate TF\n",
    "Now, we're ready to calculate the TF for each word. Use the method of your choice to find the top 5 terms within the top 5 communities (by number of authors).\n",
    "\n",
    "> First isolate the tokens of the top 5 communities. Then Cacluate TF for each token in each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 communities token count:\n",
      "14) : 287162\n",
      "15) : 626335\n",
      "9) : 187775\n",
      "21) : 541651\n",
      "2) : 245486\n"
     ]
    }
   ],
   "source": [
    "# First find top 5 comminutes by size\n",
    "top5communities = np.argsort(counts)[-5:][::-1]\n",
    "top5tokens = [communityTokens[x] for x in top5communities]\n",
    "print(\"Top 5 communities token count:\")\n",
    "for i, tokens in zip(top5communities, top5tokens):\n",
    "    print(f\"{i}) : {len(tokens)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JASON: I GET DIFFERENT RESULTS WHEN DOING FOR ALL OPPOSED TO DOING FOR TOP 5. WHY?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tf for each community\n",
    "from collections import Counter\n",
    "TF = [Counter(tokens) for tokens in top5tokens]  # non-normalized term frequency\n",
    "# Normliaze the tf\n",
    "for i, tf in enumerate(TF):\n",
    "    for key in tf:\n",
    "        tf[key] /= len(top5tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tf for ALL communities\n",
    "from collections import Counter\n",
    "TF = [Counter(tokens) for tokens in communityTokens]  # non-normalized term frequency\n",
    "# Normliaze the tf\n",
    "for i, tf in enumerate(TF):\n",
    "    for key in tf:\n",
    "        tf[key] /= len(communityTokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 terms for each community:\n",
      "0) : ('data', 'social', 'information', 'music', 'research')\n",
      "1) : ('social', 'study', 'results', 'research', 'model')\n",
      "2) : ('data', 'social', 'network', 'information', 'model')\n",
      "3) : ('social', 'media', 'political', 'data', 'news')\n",
      "4) : ('agents', 'agent', 'systems', 'paper', 'system')\n",
      "5) : ('data', 'model', 'systems', 'paper', 'social')\n",
      "6) : ('data', 'social', 'network', 'information', 'networks')\n",
      "7) : ('data', 'social', 'information', 'paper', 'results')\n",
      "8) : ('data', 'users', 'social', 'information', 'network')\n",
      "9) : ('social', 'data', 'information', 'time', 'using')\n",
      "10) : ('urban', 'systems', 'model', 'data', 'cities')\n",
      "11) : ('social', 'model', 'data', 'network', 'study')\n",
      "12) : ('data', 'information', 'system', 'systems', 'paper')\n",
      "13) : ('data', 'model', 'using', 'results', 'network')\n",
      "14) : ('social', 'data', 'work', 'users', 'study')\n",
      "15) : ('social', 'data', 'users', 'information', 'research')\n",
      "16) : ('social', 'data', 'political', 'public', 'study')\n",
      "17) : ('media', 'data', 'news', 'social', 'political')\n",
      "18) : ('data', 'de', 'network', 'results', 'networks')\n",
      "19) : ('networks', 'network', 'different', 'brain', 'results')\n",
      "20) : ('social', 'research', 'data', 'paper', 'system')\n",
      "21) : ('data', 'model', 'information', 'show', 'results')\n",
      "22) : ('data', 'model', 'models', 'graph', 'social')\n",
      "23) : ('data', 'research', 'paper', 'information', 'use')\n",
      "24) : ('security', 'data', 'network', 'system', 'game')\n",
      "25) : ('agents', 'algorithm', 'problem', 'show', 'model')\n",
      "26) : ('data', 'information', 'social', 'media', 'research')\n",
      "27) : ('model', 'data', 'results', 'using', 'research')\n",
      "28) : ('model', 'models', 'social', 'data', 'systems')\n",
      "29) : ('data', 'model', 'models', 'inference', 'using')\n",
      "30) : ('social', 'data', 'network', 'analysis', 'information')\n",
      "31) : ('data', 'social', 'information', 'using', 'study')\n"
     ]
    }
   ],
   "source": [
    "# Find the top 5 terms for each community\n",
    "top5terms = [tf.most_common(5) for tf in TF]\n",
    "top5terms = [list(zip(*terms))[0] for terms in top5terms] # Extract the terms\n",
    "\n",
    "print(\"Top 5 terms for each community:\")\n",
    "for i, terms in enumerate(top5terms):\n",
    "    print(f\"{i}) : {terms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 terms for each community:\n",
      "14) : ('data', 'social', 'information', 'music', 'research')\n",
      "15) : ('social', 'study', 'results', 'research', 'model')\n",
      "9) : ('data', 'social', 'network', 'information', 'model')\n",
      "21) : ('social', 'media', 'political', 'data', 'news')\n",
      "2) : ('agents', 'agent', 'systems', 'paper', 'system')\n"
     ]
    }
   ],
   "source": [
    "# Find the top 5 terms for each community\n",
    "top5terms = [tf.most_common(5) for tf in TF]\n",
    "top5terms = [list(zip(*terms))[0] for terms in top5terms] # Extract the terms\n",
    "\n",
    "print(\"Top 5 terms for each community:\")\n",
    "for i, terms in zip(top5communities, top5terms):\n",
    "    print(f\"{i}) : {terms}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe similarities and differences between the communities.\n",
    "> Similarities include **social, data, and information**. \n",
    "\n",
    "> Differences include **network, time, show, and results**.\n",
    "\n",
    "- Why aren't the TFs not necessarily a good description of the communities?\n",
    "> TF alone does not consider the significance of a word, i.e. words that in general appear often will unfailry score high. \n",
    "\n",
    "- Next, we calculate IDF for every word.\n",
    "> The IDF is calculated where it is assumed that each community has one document, such that N = number of communities. First determine the number of communities that contain a given word. Then calculate the IDF for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IDF for every term\n",
    "IDF = Counter()\n",
    "N = num_communities\n",
    "\n",
    "# Count the number of communities that contain a term\n",
    "for i, tf in enumerate(TF):\n",
    "    for key in tf:\n",
    "        IDF[key] += 1\n",
    "\n",
    "# Compute the IDF\n",
    "for key in IDF:\n",
    "    IDF[key] = np.log(N / IDF[key])  # natural log\n",
    "\n",
    "# Compute TF-IDF\n",
    "TFIDF = [tf.copy() for tf in TF]\n",
    "for i, tfidf in enumerate(TFIDF):\n",
    "    for key in tfidf:\n",
    "        tfidf[key] *= IDF[key]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What base logarithm did you use? Is that important?\n",
    "> Natural logirithm. Not important, we are just interested in projecting onto the log scale.\n",
    "\n",
    "## 4) TF-IDF\n",
    "We're ready to calculate TF-IDF. Do that for the top 9 communities (by number of authors). Then for each community:\n",
    "\n",
    "- List the 10 top TF words\n",
    "- List the 10 top TF-IDF words\n",
    "- List the top 3 authors (by degree)\n",
    "- Are these 10 words more descriptive of the community? If yes, what is it about IDF that makes the words more informative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 terms for each community:\n",
      "14) : ('data', 'social', 'information', 'music', 'research', 'different', 'paper', 'users', 'study', 'results')\n",
      "15) : ('social', 'study', 'results', 'research', 'model', 'crime', 'data', 'agents', 'using', 'also')\n",
      "9) : ('data', 'social', 'network', 'information', 'model', 'paper', 'mobile', 'results', 'networks', 'using')\n",
      "21) : ('social', 'media', 'political', 'data', 'news', 'information', 'study', 'research', 'public', 'networks')\n",
      "2) : ('agents', 'agent', 'systems', 'paper', 'system', 'model', 'use', 'social', 'using', 'used')\n",
      "8) : ('data', 'model', 'systems', 'paper', 'social', 'agents', 'models', 'coordination', 'different', 'system')\n",
      "6) : ('data', 'social', 'network', 'information', 'networks', 'model', 'users', 'results', 'study', 'using')\n",
      "7) : ('data', 'social', 'information', 'paper', 'results', 'model', 'using', 'different', 'problem', 'study')\n",
      "17) : ('data', 'users', 'social', 'information', 'network', 'using', 'results', 'paper', 'use', 'system')\n"
     ]
    }
   ],
   "source": [
    "# Find top 9 communities by size\n",
    "top9communities = np.argsort(counts)[-9:][::-1]\n",
    "top9tokens = [communityTokens[x] for x in top9communities]\n",
    "\n",
    "# List top 10 terms for each community\n",
    "top10terms = [tf.most_common(10) for tf in TF]\n",
    "top10terms = [list(zip(*terms))[0] for terms in top10terms] # Extract the terms\n",
    "\n",
    "print(\"Top 10 terms for each community:\")\n",
    "for i, terms in zip(top9communities, top10terms):\n",
    "    print(f\"{i}) : {terms}\")\n",
    "\n",
    "# Compute TF-IDF\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authorID</th>\n",
       "      <th>group</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2101037</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3001795</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2080155085</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33570565</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66118125</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>10852593</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>1734917</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>9486542</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>144188281</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>40827815</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1271 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        authorID  group  degree\n",
       "0        2101037      0      10\n",
       "1        3001795      0       2\n",
       "2     2080155085      0       2\n",
       "3       33570565      0       4\n",
       "4       66118125      0       3\n",
       "...          ...    ...     ...\n",
       "1266    10852593      8       2\n",
       "1267     1734917      7       1\n",
       "1268     9486542     14       2\n",
       "1269   144188281     27       1\n",
       "1270    40827815      9       2\n",
       "\n",
       "[1271 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# GARBAGE CODE\n",
    "\n",
    "import pandas as pd\n",
    "degree = dict(G.degree())\n",
    "nx.set_node_attributes(G, degree, name=\"degree\") \n",
    "df=pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')\n",
    "df = df.reset_index(names=\"authorID\")\n",
    "df = df[['authorID','group', 'degree']]  # Only interested in subset of columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
